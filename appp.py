import streamlit as st
import feedparser
import urllib.parse
import os
import time
import requests
import urllib3
import sqlite3
import pandas as pd
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from newspaper import Article, Config
import nltk
import google.generativeai as genai

# -------------------------------------------
# 0. API í‚¤ ë° ì´ˆê¸° ì„¤ì •
# -------------------------------------------

GOOGLE_API_KEY = "AIzaSyAdnBk6ZdKpxL98LHHaGj9Bjbfk_dX81DA" 

try:
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')
except Exception as e:
    st.error(f"API í‚¤ ì„¤ì • ì˜¤ë¥˜: {e}")

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')

HISTORY_FILE = "seen_titles.txt"
DB_FILE = "news_database.db"

# -------------------------------------------
# 1. ìœ í‹¸ë¦¬í‹° ë° DB í•¨ìˆ˜ë“¤
# -------------------------------------------

def get_final_url(url):
    """êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ URLì„ ì‹¤ì œ ë‰´ìŠ¤ URLë¡œ ë³€í™˜"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=5, allow_redirects=True, verify=False)
        return response.url
    except Exception:
        return url

def fetch_rss_feed(url):
    """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        response = requests.get(url, headers=headers, timeout=10, verify=False)
        if response.status_code != 200:
            return None
        return feedparser.parse(response.content)
    except Exception as e:
        return None

def is_within_1hour(published_str):
    """
    ê¸°ì‚¬ ë°œí–‰ ì‹œê°„ì„ í™•ì¸í•˜ì—¬ 1ì‹œê°„ ì´ë‚´ì¸ì§€ íŒë³„í•˜ëŠ” í•¨ìˆ˜
    """
    if not published_str:
        return False
    try:
        # RSS ë‚ ì§œ íŒŒì‹± (GMT ê¸°ì¤€)
        pub_dt = parsedate_to_datetime(published_str)
        
        # í˜„ì¬ ì‹œê°„ (UTCë¡œ í†µì¼í•˜ì—¬ ê³„ì‚°)
        now_dt = datetime.now(timezone.utc)
        
        # ì°¨ì´ ê³„ì‚°
        diff = now_dt - pub_dt
        
        # 3600ì´ˆ(1ì‹œê°„) ì´ë‚´ë©´ True, ì•„ë‹ˆë©´ False
        # (ì•½ê°„ì˜ ì˜¤ì°¨ í—ˆìš©ì„ ìœ„í•´ 65ë¶„ê¹Œì§€ ì—¬ìœ ë¥¼ ë‘ )
        if diff.total_seconds() <= 3900: 
            return True
        return False
    except:
        return False # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì œì™¸

def init_db():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute('''
        CREATE TABLE IF NOT EXISTS saved_news (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            keyword TEXT,
            title TEXT,
            link TEXT,
            pub_date TEXT,
            saved_at TEXT
        )
    ''')
    conn.commit()
    conn.close()

def save_news_to_db(keyword, title, link, pub_date):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("SELECT id FROM saved_news WHERE title = ? AND link = ?", (title, link))
    if c.fetchone():
        conn.close()
        return False
    saved_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    c.execute("INSERT INTO saved_news (keyword, title, link, pub_date, saved_at) VALUES (?, ?, ?, ?, ?)",
              (keyword, title, link, pub_date, saved_at))
    conn.commit()
    conn.close()
    return True

def get_saved_news():
    conn = sqlite3.connect(DB_FILE)
    df = pd.read_sql_query("SELECT * FROM saved_news ORDER BY saved_at DESC", conn)
    conn.close()
    return df

def delete_news_from_db(news_ids):
    if not news_ids: return
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    placeholders = ', '.join('?' for _ in news_ids)
    c.execute(f"DELETE FROM saved_news WHERE id IN ({placeholders})", news_ids)
    conn.commit()
    conn.close()

init_db()

def load_seen_titles():
    if not os.path.exists(HISTORY_FILE):
        return set()
    with open(HISTORY_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f)

def save_seen_title(title):
    clean_title = title.replace("\n", " ")
    with open(HISTORY_FILE, "a", encoding="utf-8") as f:
        f.write(clean_title + "\n")

def format_date_kor(date_str):
    try:
        if not date_str: return "ì‹œê°„ ì •ë³´ ì—†ìŒ"
        dt = parsedate_to_datetime(date_str)
        KST = timezone(timedelta(hours=9))
        dt_kst = dt.astimezone(KST)
        return dt_kst.strftime("%Yë…„ %mì›” %dì¼ %H:%M")
    except:
        return date_str[:16]

def get_current_time_str():
    now = datetime.now()
    return now.strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„ %Sì´ˆ")

# -------------------------------------------
# 2. í™”ë©´ êµ¬ì„± (UI)
# -------------------------------------------
st.set_page_config(page_title="ê¸°ì—… ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§", page_icon="ğŸ’»", layout="wide")

if 'selected_article_url' not in st.session_state:
    st.session_state['selected_article_url'] = None
if 'selected_article_title' not in st.session_state:
    st.session_state['selected_article_title'] = None

with st.sidebar:
    st.header("âš™ï¸ ëª¨ë‹ˆí„°ë§ ì„¤ì •")
    default_keywords = "ë¡¯ë°ë§ˆíŠ¸, ë¡¯ë°ì›°í‘¸ë“œ, [ë‹¨ë…]ë¡¯ë°, ë¡¯ë°ì¹ ì„±, ì„¸ë¸ì¼ë ˆë¸, ì‚¼ì„±"
    user_input = st.text_area("í‚¤ì›Œë“œ ì…ë ¥ (ì½¤ë§ˆ êµ¬ë¶„)", value=default_keywords, height=100)
    
    KEYWORDS = [k.strip() for k in user_input.split(',') if k.strip()]
    
    st.divider()
    
    st.subheader("â±ï¸ ìë™ ì—…ë°ì´íŠ¸")
    auto_refresh = st.checkbox("ìë™ ìƒˆë¡œê³ ì¹¨ ì¼œê¸°", value=True)
    refresh_interval = st.slider("ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë¶„)", 5, 60, 15)
    
    if st.button("ğŸ—‘ï¸ ê¸°ë¡ ì´ˆê¸°í™”"):
        if os.path.exists(HISTORY_FILE):
            os.remove(HISTORY_FILE)
            st.rerun()

st.title("ğŸ’» ì‹¤ì‹œê°„ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ (Gemini AI)")

# -------------------------------------------
# 3. ë©”ì¸ ë¡œì§
# -------------------------------------------
tab1, tab2, tab3 = st.tabs(["ğŸ“¢ ë‰´ìŠ¤ ëª©ë¡ (1ì‹œê°„ ì´ë‚´)", "ğŸ“ AI ìƒì„¸ ìš”ì•½", "ğŸ—„ï¸ ì €ì¥ì†Œ (DB)"])

# === [íƒ­ 1] ë‰´ìŠ¤ ëª©ë¡ ===
with tab1:
    status_container = st.container()
    
    seen_titles = load_seen_titles()
    grouped_news = {k: [] for k in KEYWORDS}
    new_news_count = 0 
    
    with st.spinner("1ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ë¥¼ ì •ë°€ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
        for keyword in KEYWORDS:
            clean_keyword = keyword.strip()
            
            # [ì „ëµ ë³€ê²½] 
            # êµ¬ê¸€ì—ëŠ” '12ì‹œê°„(when:12h)' ë°ì´í„°ë¥¼ ìš”ì²­í•´ì„œ ë„‰ë„‰í•˜ê²Œ ë°›ì•„ì˜µë‹ˆë‹¤.
            # ê·¸ í›„, ì•„ë˜ ì½”ë“œ(is_within_1hour)ì—ì„œ 1ì‹œê°„ ì´ë‚´ì¸ ê²ƒë§Œ ì‚´ë¦½ë‹ˆë‹¤.
            search_query = clean_keyword + " when:12h"
            encoded_keyword = urllib.parse.quote(search_query)
            rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
            
            feed = fetch_rss_feed(rss_url)
            
            if not feed or not feed.entries:
                continue

            for entry in feed.entries:
                # [ì—¬ê¸°ì„œ í•„í„°ë§] 1ì‹œê°„ ì´ë‚´ê°€ ì•„ë‹ˆë©´ ê³¼ê°íˆ ìŠ¤í‚µ
                if not is_within_1hour(entry.get('published', '')):
                    continue
                
                title = entry.title
                link = entry.link
                nice_date = format_date_kor(entry.get('published', ''))
                
                if clean_keyword not in title: continue
                
                grouped_news[clean_keyword].append({
                    "title": title, "link": link, "date": nice_date
                })
                
                if title not in seen_titles:
                    seen_titles.add(title)
                    save_seen_title(title)
                    new_news_count += 1

    current_time = get_current_time_str()
    total_news = sum(len(items) for items in grouped_news.values())

    if new_news_count > 0:
        status_container.success(f"ğŸ”¥ **ì—…ë°ì´íŠ¸ ì™„ë£Œ ({current_time})** : {new_news_count}ê±´ì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤!")
        st.toast(f"{new_news_count}ê±´ì˜ ìƒˆ ë‰´ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤!", icon="ğŸ”¥")
    elif total_news > 0:
        status_container.info(f"âœ… **ì—…ë°ì´íŠ¸ ì™„ë£Œ ({current_time})** : 1ì‹œê°„ ì´ë‚´ ìƒˆë¡œìš´ ë‰´ìŠ¤ëŠ” ì—†ì§€ë§Œ, ê¸°ì¡´ {total_news}ê±´ì´ í‘œì‹œë©ë‹ˆë‹¤.")
    else:
        status_container.warning(f"âš ï¸ **ê²€ìƒ‰ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤** ({current_time}) - ìµœê·¼ 1ì‹œê°„ ì´ë‚´ì— ë°œí–‰ëœ ê¸°ì‚¬ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    btn_idx = 0 
    for keyword, items in grouped_news.items():
        if items: 
            with st.expander(f"ğŸ“‚ **{keyword}** ({len(items)}ê±´)", expanded=True):
                for item in items:
                    with st.container():
                        c1, c2, c3, c4 = st.columns([1.2, 3.5, 0.8, 0.8])
                        c1.markdown(f":orange[{item['date']}]")
                        c2.markdown(f"[{item['title']}]({item['link']})")
                        
                        if c3.button("ğŸ“ ìš”ì•½", key=f"btn_sum_{btn_idx}"):
                            st.session_state['selected_article_url'] = item['link']
                            st.session_state['selected_article_title'] = item['title']
                            st.toast("íƒ­ 2ë¡œ ì´ë™í•˜ì„¸ìš”!", icon="ğŸ‘‰")
                        
                        if c4.button("ğŸ’¾ ì €ì¥", key=f"btn_save_{btn_idx}"):
                            success = save_news_to_db(keyword, item['title'], item['link'], item['date'])
                            if success:
                                st.toast("ì €ì¥ì†Œ(DB)ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!", icon="âœ…")
                            else:
                                st.toast("ì´ë¯¸ ì €ì¥ëœ ë‰´ìŠ¤ì…ë‹ˆë‹¤.", icon="âš ï¸")
                                
                        btn_idx += 1
                    st.divider()

# === [íƒ­ 2] AI ìš”ì•½ ===
with tab2:
    st.header("ğŸ“ Gemini ê¸°ì‚¬ ìš”ì•½")
    selected_url = st.session_state['selected_article_url']
    
    if selected_url is None:
        st.info("ğŸ‘ˆ [ë‰´ìŠ¤ ëª©ë¡] íƒ­ì—ì„œ 'AI ìš”ì•½' ë²„íŠ¼ì„ ë¨¼ì € ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        st.subheader(f"{st.session_state['selected_article_title']}")
        st.markdown("---")
        
        with st.spinner("ğŸ”— ì‹¤ì œ ê¸°ì‚¬ ì£¼ì†Œë¥¼ ì°¾ëŠ” ì¤‘..."):
            final_url = get_final_url(selected_url)
        
        with st.spinner(f"Geminiê°€ ê¸°ì‚¬ë¥¼ ì½ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                config = Config()
                config.request_timeout = 10
                config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                config.request_kwargs = {'verify': False}
                
                article = Article(final_url, language='ko', config=config)
                article.download()
                article.parse()
                
                if article.top_image:
                    st.image(article.top_image, use_container_width=True)

                if len(article.text) < 50:
                    st.warning("âš ï¸ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë³´ì•ˆì´ ê°•í•œ ì–¸ë¡ ì‚¬ì´ê±°ë‚˜ ìœ ë£Œ ê¸°ì‚¬ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                    st.write(f"ë³€í™˜ëœ ë§í¬: {final_url}")
                else:
                    prompt = f"""
                    ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì½ê³  ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:
                    1. **í•œì¤„ ìš”ì•½**: ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œ
                    2. **ìƒì„¸ í¬ì¸íŠ¸**: ì¤‘ìš” ë‚´ìš© 3ê°€ì§€ (ê¸€ë¨¸ë¦¬ ê¸°í˜¸)
                    3. **ê°ì • ë¶„ì„**: ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì¤‘ í•˜ë‚˜
                    
                    [ê¸°ì‚¬ ë³¸ë¬¸]
                    {article.text[:3000]}
                    """
                    response = model.generate_content(prompt)
                    st.success(response.text)

                with st.expander("ì›ë³¸ ë³¸ë¬¸ ë³´ê¸°"):
                    st.write(article.text)
                    
            except Exception as e:
                st.error("ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                st.caption(f"Error: {e}")

# === [íƒ­ 3] ì €ì¥ì†Œ (DB) ===
with tab3:
    st.header("ğŸ—„ï¸ ì €ì¥ëœ ë‰´ìŠ¤ ê´€ë¦¬")
    st.caption("ì˜êµ¬ ì €ì¥ëœ ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ê³  ì—‘ì…€ë¡œ ë‚´ë³´ë‚´ê±°ë‚˜ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    df = get_saved_news()
    
    if df.empty:
        st.info("ì•„ì§ ì €ì¥ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. 'ë‰´ìŠ¤ ëª©ë¡' íƒ­ì—ì„œ 'ğŸ’¾ ì €ì¥' ë²„íŠ¼ì„ ëˆŒëŸ¬ë³´ì„¸ìš”.")
    else:
        st.subheader(f"ì´ {len(df)}ê±´ì˜ ìŠ¤í¬ë©")
        
        df_display = df
