import streamlit as st
import feedparser
import urllib.parse
import os
import time
import requests
import urllib3
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from newspaper import Article, Config
import nltk
import google.generativeai as genai

# -------------------------------------------
# 0. ì´ˆê¸° ì„¤ì • ë° API ì—°ê²°
# -------------------------------------------
GOOGLE_API_KEY = "AIzaSyAdnBk6ZdKpxL98LHHaGj9Bjbfk_dX81DA" 

try:
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')
except Exception as e:
    st.error(f"API í‚¤ ì„¤ì • ì˜¤ë¥˜: {e}")

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# NLTK ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')

HISTORY_FILE = "seen_titles.txt"

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (í•„ìˆ˜)
if 'selected_article_url' not in st.session_state:
    st.session_state['selected_article_url'] = None
if 'selected_article_title' not in st.session_state:
    st.session_state['selected_article_title'] = None

# -------------------------------------------
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# -------------------------------------------
def load_seen_titles():
    if not os.path.exists(HISTORY_FILE):
        return set()
    with open(HISTORY_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f)

def save_seen_title(title):
    clean_title = title.replace("\n", " ")
    with open(HISTORY_FILE, "a", encoding="utf-8") as f:
        f.write(clean_title + "\n")

def format_date_kor(date_str):
    try:
        if not date_str: return "ì‹œê°„ ì •ë³´ ì—†ìŒ"
        dt = parsedate_to_datetime(date_str)
        KST = timezone(timedelta(hours=9))
        dt_kst = dt.astimezone(KST)
        return dt_kst.strftime("%Yë…„ %mì›” %dì¼ %H:%M")
    except:
        return date_str[:16]

def get_current_time_str():
    now = datetime.now()
    return now.strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„ %Sì´ˆ")

# í•¨ìˆ˜ ë‚´ UI ì½”ë“œë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜í•˜ê²Œ RSSë§Œ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •
def fetch_rss_feed(url):
    try:
        feed = feedparser.parse(url)
        return feed
    except Exception as e:
        st.error(f"RSS ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# -------------------------------------------
# 2. ì‚¬ì´ë“œë°” / ì„¤ì • ì˜ì—­
# -------------------------------------------
st.title("ğŸ’» ì‹¤ì‹œê°„ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§")

# ì„¤ì • ì…ë ¥ì°½ë“¤ì„ í•¨ìˆ˜ ë°–ìœ¼ë¡œ ë°°ì¹˜
default_keywords = "ì‚¼ì„±ì „ì, ì—”ë¹„ë””ì•„, ë¹„íŠ¸ì½”ì¸"
user_input = st.text_area("í‚¤ì›Œë“œ ì…ë ¥ (ì½¤ë§ˆ êµ¬ë¶„)", value=default_keywords, height=100)
KEYWORDS = [k.strip() for k in user_input.split(',') if k.strip()]

col1, col2 = st.columns(2)
with col1:
    auto_refresh = st.checkbox("ìë™ ìƒˆë¡œê³ ì¹¨ ì¼œê¸°", value=True)
with col2:
    refresh_interval = st.slider("ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë¶„)", 5, 60, 15)

if st.button("ğŸ—‘ï¸ ê¸°ë¡ ì´ˆê¸°í™”"):
    if os.path.exists(HISTORY_FILE):
        os.remove(HISTORY_FILE)
        st.rerun()

st.divider()

# -------------------------------------------
# 3. ë©”ì¸ ë¡œì§
# -------------------------------------------
tab1, tab2 = st.tabs(["ğŸ“¢ ë‰´ìŠ¤ ëª©ë¡", "ğŸ“ AI ìƒì„¸ ìš”ì•½"])

# === [íƒ­ 1] ë‰´ìŠ¤ ëª©ë¡ ===
with tab1:
    status_container = st.empty() # ìƒíƒœ í‘œì‹œë¥¼ ìœ„í•œ ë¹ˆ ì»¨í…Œì´ë„ˆ
    seen_titles = load_seen_titles()
    grouped_news = {k: [] for k in KEYWORDS}
    new_news_count = 0 
    
    for keyword in KEYWORDS:
        search_query = f"{keyword} when:1h"
        encoded_query = urllib.parse.quote(search_query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        
        feed = fetch_rss_feed(rss_url)
        
        if not feed or not feed.entries:
            continue

        for entry in feed.entries:
            title = entry.title
            link = entry.link
            nice_date = format_date_kor(entry.get('published', ''))
            
            # í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ëœ ê²½ìš°ë§Œ í•„í„°ë§
            if keyword.lower() in title.lower():
                grouped_news[keyword].append({
                    "title": title, "link": link, "date": nice_date
                })
                
                if title not in seen_titles:
                    seen_titles.add(title)
                    save_seen_title(title)
                    new_news_count += 1

    # ì—…ë°ì´íŠ¸ ê²°ê³¼ í‘œì‹œ
    current_time = get_current_time_str()
    if new_news_count > 0:
        status_container.success(f"ğŸ”¥ **ì—…ë°ì´íŠ¸ ì™„ë£Œ ({current_time})** : {new_news_count}ê±´ì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤!")
    else:
        status_container.info(f"âœ… **ì—…ë°ì´íŠ¸ ì™„ë£Œ ({current_time})** : ìƒˆë¡œìš´ ì†Œì‹ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ë‰´ìŠ¤ ì¹´ë“œ ì¶œë ¥
    btn_idx = 0 
    for keyword, items in grouped_news.items():
        if items: 
            with st.expander(f"ğŸ“‚ **{keyword}** ({len(items)}ê±´)", expanded=True):
                for item in items:
                    c1, c2, c3 = st.columns([1.5, 4, 1])
                    c1.caption(item['date'])
                    c2.markdown(f"[{item['title']}]({item['link']})")
                    if c3.button("ğŸ“ ìš”ì•½", key=f"btn_{btn_idx}"):
                        st.session_state['selected_article_url'] = item['link']
                        st.session_state['selected_article_title'] = item['title']
                        st.rerun() # íƒ­ ì´ë™ ì•ˆë‚´ ëŒ€ì‹  ì¦‰ì‹œ ë°˜ì˜
                    btn_idx += 1
                    st.divider()

# === [íƒ­ 2] AI ìš”ì•½ ===
with tab2:
    st.header("ğŸ“ Gemini ê¸°ì‚¬ ìš”ì•½")
    selected_url = st.session_state.get('selected_article_url')
    
    if not selected_url:
        st.info("ğŸ‘ˆ [ë‰´ìŠ¤ ëª©ë¡] íƒ­ì—ì„œ 'ìš”ì•½' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        st.subheader(f"ğŸ” {st.session_state['selected_article_title']}")
        
        with st.spinner("AIê°€ ë¶„ì„ ì¤‘..."):
            try:
                config = Config()
                config.request_timeout = 10
                article = Article(selected_url, language='ko', config=config)
                article.download()
                article.parse()
                
                if article.top_image:
                    st.image(article.top_image, use_container_width=True)

                if len(article.text) < 50:
                    st.warning("ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
                    st.write(article.text)
                else:
                    prompt = f"ë‹¤ìŒ ê¸°ì‚¬ë¥¼ [í•œì¤„ ìš”ì•½], [3ê°€ì§€ í•µì‹¬ í¬ì¸íŠ¸], [ê¸ë¶€ì • ë¶„ì„] ìˆœìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n\n{article.text[:3000]}"
                    response = model.generate_content(prompt)
                    st.markdown(response.text)

                with st.expander("ì›ë³¸ ë³¸ë¬¸ ë³´ê¸°"):
                    st.write(article.text)
                    
            except Exception as e:
                st.error(f"ìš”ì•½ ì‹¤íŒ¨: {e}")

# ìë™ ìƒˆë¡œê³ ì¹¨ ë¡œì§
if auto_refresh:
    time.sleep(2) # ì¦‰ì‹œ ì¬ì‹¤í–‰ ë°©ì§€ (ë¬´í•œë£¨í”„ ë°©ì§€)
    st.empty() # í™”ë©´ ìœ ì§€
    # ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” st.empty()ì™€ sleepì„ ì¡°í•©í•œ ë³„ë„ ë¡œì§ ê¶Œì¥
